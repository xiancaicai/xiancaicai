{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Here, the pre-trained deep neural language model BERT (Bidirectional Encoder Representations from Transformers) is used to classify sequences from the HP corpora.\n",
        "\n",
        "To pre-process the corpus data for it to be used with the BERT model, the text of each book has been split into shorter sequences of approximately 6 linguistic sentences which have then been labeled according to the book to which they pertain.\n",
        "\n",
        "The data consisting of a total of 6603 sequences has been\n",
        "split into a training (80%) and a test (20%) set."
      ],
      "metadata": {
        "id": "0wNMyAMpX-on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "-AAoJNAwvSxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): #Use GPU if available\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU used:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('CPU used.')"
      ],
      "metadata": {
        "id": "BbM3MI8iEVqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Hugging Face ðŸ¤— Transformers**"
      ],
      "metadata": {
        "id": "1H524V39u9aI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JphIdD6NS53"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training data**"
      ],
      "metadata": {
        "id": "X7Z3sMEFNwGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload() #Select and upload the relevant file"
      ],
      "metadata": {
        "id": "YSl2Voq2NlP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Train.csv\")\n",
        "print(f\"Total number of sentence sequences: {len(df)}\")\n",
        "df"
      ],
      "metadata": {
        "id": "78HgVlgZNk7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Histogram: Number of Sequences per Book\n",
        "category_count = df['Book'].value_counts()\n",
        "categories = category_count.index\n",
        "\n",
        "fig = plt.figure(figsize= (12, 5))\n",
        "plt.style.use('ggplot')\n",
        "ax = fig.add_subplot(121)\n",
        "sns.barplot(x = categories, y = category_count)\n",
        "for a, p in enumerate(ax.patches):\n",
        "    ax.annotate(f'' + format(p.get_height(), '.0f'),\n",
        "                xy = (p.get_x() + p.get_width() / 2.0, p.get_height()),\n",
        "                xytext = (0,-25), size = 13, color = 'white' , ha = 'center',\n",
        "                va = 'center', textcoords = 'offset points',\n",
        "                bbox = dict(boxstyle = 'round', facecolor='none', edgecolor='white', alpha = 0.5))\n",
        "\n",
        "plt.xlabel('Book', size = 15)\n",
        "plt.ylabel('Sequences', size= 15)\n",
        "plt.xticks(size = 12)\n",
        "plt.title(\"Number of Sequences per Book\" , size = 18)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9vP7GVLBR-eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT Tokenizer**"
      ],
      "metadata": {
        "id": "ZkFi1r3ubTIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "sentences = df.Sentences.values\n",
        "label = df.Book.values\n",
        "\n",
        "print(sentences[21]) #original sentence\n",
        "print(tokenizer.tokenize(sentences[21])) #sentence split into tokens\n",
        "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[21]))) #sentence mapped to token ids"
      ],
      "metadata": {
        "id": "uiUT7GB3bMWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing all sentences and mapping tokens to word IDs\n",
        "input_id = []\n",
        "\n",
        "for s in sentences:\n",
        "    encoded_s = tokenizer.encode(s,                      \n",
        "                        add_special_tokens = True) #[CLS],[SEP]\n",
        "    input_id.append(encoded_s)\n",
        "\n",
        "print('Max sentence length: ', max([len(s) for s in input_id]))"
      ],
      "metadata": {
        "id": "Pa9jytARbMzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Padding/Truncating\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_len = 250\n",
        "input_id = pad_sequences(input_id, maxlen=max_len, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\") #Pad input tokens with value 0. Post: end of sequence.\n",
        "\n",
        "attention_mask = []\n",
        "for s in input_id:\n",
        "    att_m = [int(token_id > 0) for token_id in s] #padding, ID = 0 ->  mask = 0. token, ID > 0 -> mask = 1.\n",
        "    attention_mask.append(att_m)"
      ],
      "metadata": {
        "id": "Vi7z-eBNeD3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting into train and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_input, validation_input, train_label, validation_label = train_test_split(input_id, label, \n",
        "                                                            random_state=2023, test_size=0.1) #90% train, 10% val.\n",
        "train_mask, validation_mask, _, _ = train_test_split(attention_mask, label,\n",
        "                                             random_state=2023, test_size=0.1)\n",
        "\n",
        "#Converting into torch tensors\n",
        "train_input = torch.tensor(train_input)\n",
        "validation_input = torch.tensor(validation_input)\n",
        "train_label = torch.tensor(train_label)\n",
        "validation_label = torch.tensor(validation_label)\n",
        "train_mask = torch.tensor(train_mask)\n",
        "validation_mask = torch.tensor(validation_mask)"
      ],
      "metadata": {
        "id": "VhsmLtQ5f2CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 16\n",
        "\n",
        "train_data = TensorDataset(train_input, train_mask, train_label)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "val_data = TensorDataset(validation_input, validation_mask, validation_label)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "1kTjPek-gw4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT Classification**\n",
        "\n",
        "BertForSequenceClassification: pretrained BERT model with added single linear classification layer."
      ],
      "metadata": {
        "id": "TDYdERk0hHaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", #12 layer BERT\n",
        "    num_labels = 7,  \n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "model.cuda() #run model on GPU"
      ],
      "metadata": {
        "id": "sOSkZoTPhKIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizer**\n",
        "AdamW: weight decay"
      ],
      "metadata": {
        "id": "ECs22ZkyiaPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                  lr = 5e-5,\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "\n",
        "epochs = 4 #Training epochs\n",
        "total_steps = len(train_dataloader) * epochs #nr batches * nrepochs\n",
        "\n",
        "#Learning rate scheduler.\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "KTknbd5GifC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "    \n",
        "def flat_accuracy(prediction, label):\n",
        "    '''\n",
        "    Calculates accuracy of predictions\n",
        "    '''\n",
        "    pred_flat = np.argmax(prediction, axis=1).flatten()\n",
        "    label_flat = label.flatten()\n",
        "    return np.sum(pred_flat == label_flat) / len(label_flat)"
      ],
      "metadata": {
        "id": "f9LxOW0niiAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**\n",
        "Based on [run_glue.py](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128)"
      ],
      "metadata": {
        "id": "cCZKBfGLi9K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "seed_val = 2023\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "loss_val = []\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "\n",
        "    #Training\n",
        "    print('Epoch {:} / {:} '.format(epoch + 1, epochs))\n",
        "    print('Training...')\n",
        "    \n",
        "    t0 = time.time() #time for training epoch\n",
        "    total_loss = 0 #reset total loss for epoch\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 40 == 0 and not step == 0: #progress update every 40 batches\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        #unpacking training batch fom dataloader\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad() #remove previously calculated gradients        \n",
        "\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        total_loss += loss.item()\n",
        "        loss.backward() #backward pass to calculate gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #preventing exploding gradients\n",
        "\n",
        "        optimizer.step() #update parameters taking step using gradient\n",
        "        lr_scheduler.step() #update learning rate\n",
        "\n",
        "    train_loss = total_loss / len(train_dataloader)\n",
        "    loss_val.append(train_loss)\n",
        "\n",
        "    print(\"Average training loss: {0:.2f}\".format(train_loss))\n",
        "    print(\"Training epoch time: {:}\".format(format_time(time.time() - t0)))\n",
        "    \n",
        "    #Validation\n",
        "\n",
        "    print(\"Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    batches_eval_steps, batches_eval_examples = 0, 0\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "            #no computation or storage of gradients to save memory and speed up validation\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        logits = outputs[0] #values prior to applying activation function\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_id = b_labels.to('cpu').numpy()\n",
        "    \n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_id)\n",
        "        eval_accuracy += tmp_eval_accuracy #total accuracy\n",
        "\n",
        "        batches_eval_steps += 1 #nr batches\n",
        "\n",
        "    print(\"Accuracy: {0:.2f}\".format(eval_accuracy/batches_eval_steps))\n",
        "    print(\"Validation time: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "BuzXKamei3z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot training loss\n",
        "sns.set(style='darkgrid')\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "plt.plot(loss_val, 'b-o')\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DCf9CqrrgS2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Performance On Test Set**"
      ],
      "metadata": {
        "id": "E9Sswzx2hN60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload test set\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "2zg3y2mqdbIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect data\n",
        "df_test = pd.read_csv(\"Test.csv\")\n",
        "print(f\"Total number of sentence sequences: {len(df_test)}\")\n",
        "df_test"
      ],
      "metadata": {
        "id": "NoGr8jbguKB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of test sentences: {:,}\\n'.format(df_test.shape[0]))\n",
        "sentences = df_test.Sentences.values\n",
        "label = df_test.Book.values\n",
        "input_id = []\n",
        "\n",
        "for s in sentences:\n",
        "    encoded_s = tokenizer.encode(s,\n",
        "                        add_special_tokens = True,\n",
        "                   )\n",
        "    \n",
        "    input_id.append(encoded_s)\n",
        "\n",
        "input_id = pad_sequences(input_id, maxlen=max_len, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "attention_mask = []\n",
        "for seq in input_id:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_mask.append(seq_mask) \n",
        "\n",
        "prediction_input = torch.tensor(input_id)\n",
        "prediction_mask = torch.tensor(attention_mask)\n",
        "prediction_label = torch.tensor(label)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "prediction_data = TensorDataset(prediction_input, prediction_mask, prediction_label)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "w7NBwEpIuNo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        " \n",
        "predictions, true_labels = [], []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      #Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('Done!')"
      ],
      "metadata": {
        "id": "WRlLIojVuUUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy measured by Matthews correlation coefficient\n",
        "(MCC) because of class imbalance.**\n",
        "\n",
        "The MCC is  a correlation coefficient between actual and\n",
        "predicted classifications and range between -1 and 1 where\n",
        "a value of 1 indicates perfect prediction and a value of 0 indicates\n",
        "that the prediction is as good as a random prediction."
      ],
      "metadata": {
        "id": "BKgb8-dyuXou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  pred_label = np.argmax(predictions[i], axis=1).flatten() \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_label)                \n",
        "  matthews_set.append(matthews)\n",
        "\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist] #combine to single lsit\n",
        "\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %3f' % mcc)"
      ],
      "metadata": {
        "id": "x_b4qbiouair"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(flat_true_labels,flat_predictions))\n",
        "\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(flat_true_labels, flat_predictions))\n",
        "\n",
        "print(\"Accuracy:\")\n",
        "print(accuracy_score(flat_true_labels, flat_predictions))"
      ],
      "metadata": {
        "id": "YhT05ddCSrC_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}